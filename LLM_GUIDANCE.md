# How the LLM Gets Guidance on MDC File Structure

This document explains the mechanism by which the LLM (Large Language Model) understands and generates MDC (Markdown Context) files with the correct structure for Cursor IDE.

## Overview

The MDC generator uses a three-part approach to guide the LLM in creating properly structured MDC files:

1. **Pydantic Model Schema** - Defines the required structure and fields
2. **User Prompts** - Provides explicit instructions and context
3. **Structured Output** - Enforces the model schema as the response format

## 1. Pydantic Model Schema (MDCResponse)

Located in `cursor_mdc_generator/llm_utils/models.py`, the `MDCResponse` model defines the exact structure of an MDC file:

```python
class MDCResponse(BaseModel):
    """Model for structured MDC file content generation."""

    description: str = Field(
        ..., description="A brief description of what this rule provides context for."
    )
    globs: list[str] = Field(
        ..., description="File patterns this rule applies to, using glob syntax."
    )
    always_apply: bool = Field(
        ...,
        description="Whether this rule should always be applied regardless of file context. Generally this should be false other than core rules.",
    )
    content: str = Field(
        ...,
        description="The markdown content providing useful documentation and context.",
    )
```

### Key Features:

- **Field Descriptions**: Each field has a description that instructs the LLM on its purpose
- **Type Validation**: Pydantic enforces correct data types (str, list[str], bool)
- **Required Fields**: All fields are required (marked with `...`)
- **Clear Semantics**: Field names are self-explanatory (description, globs, always_apply, content)

## 2. User Prompts

The system generates detailed prompts that explicitly instruct the LLM on what to include in each field. There are three types of prompts:

### A. File-Level Prompts

Generated by `format_file_prompt()` in `cursor_mdc_generator/llm_utils/prompts.py`:

```python
prompt += """
Based on the file content and dependency information, create a .mdc file for Cursor IDE with:

1. A concise description field explaining what this file does
2. Appropriate glob patterns (use: {file_path})
3. Whether this rule should always apply (typically false)
4. Detailed markdown content with:
   - Overview of the file's purpose and functionality
   - Description of key components (functions, classes, etc.)
   - How this file relates to other files in the codebase (dependencies)
   - Usage examples where appropriate
   - Best practices when working with this code

The output should help developers quickly understand this file and its role in the larger codebase.
"""
```

### B. Directory-Level Prompts

Generated by `format_directory_prompt()`:

```python
prompt += """
Based on the directory content and dependency information, create a .mdc file for Cursor IDE with:

1. A concise description field explaining what this directory contains
2. Appropriate glob patterns (use: {directory}/*)
3. Whether this rule should always apply (typically false)
4. Detailed markdown content with:
   - Overview of the directory's purpose
   - Summary of key files and their roles
   - How this directory relates to other parts of the codebase
   - Common patterns or conventions used
   - Best practices when working with files in this directory

The output should help developers quickly understand the purpose and organization of this directory.
"""
```

### C. Repository-Level Prompts

Generated by `format_repository_prompt()`:

```python
prompt += """
Based on the repository structure and dependency information, create a .mdc file for Cursor IDE with:

1. A concise description field explaining what this repository does
2. Appropriate glob patterns (use: *)
3. Whether this rule should always apply (typically true for repository-wide documentation)
4. Detailed markdown content with:
   - Overview of the repository's purpose
   - Summary of key directories and their roles
   - Architectural patterns and organization
   - Core modules and their significance
   - Entry points and how to navigate the codebase
   - Best practices for working with this repository

The output should help developers quickly understand the overall structure and organization of the codebase.
"""
```

## 3. Structured Output Mechanism

The LLM client in `cursor_mdc_generator/llm_utils/llm_client.py` uses the `MDCResponse` model as the `response_format` parameter:

```python
async def generate_response(
    messages: List[Dict[str, str]],
    model_name: str = "gpt-4o-mini",
    response_model: Optional[BaseModel] = None,
    temperature: float = 0.3,
    max_tokens: Optional[int] = None,
) -> Any:
    """Generate a response using the litellm Router."""
    
    model_kwargs = {"temperature": temperature}
    
    if response_model:
        model_kwargs["response_format"] = response_model
    
    response = await router.acompletion(
        model=model_name, messages=messages, **model_kwargs
    )
    response, cost = text_cost_parser(response)
    datamodel = response_model(**json.loads(response))
    return datamodel
```

### How It Works:

1. **Response Format**: The `response_format` parameter tells the LLM to output JSON that matches the Pydantic model
2. **Validation**: The response is parsed and validated against the `MDCResponse` model
3. **Type Safety**: Any response that doesn't match the schema will raise a validation error
4. **Automatic Parsing**: The JSON response is automatically converted to a Python object

## 4. System Prompt

A simple but effective system prompt sets the role:

```python
SYSTEM_PROMPT = "You are an expert code documentation specialist."
```

This establishes the LLM's expertise domain, ensuring it approaches the task with the right mindset.

## 5. Complete Flow

Here's how it all comes together:

```
1. Code Analysis
   └─> File/Directory/Repository content is analyzed
   
2. Context Gathering
   └─> Dependencies, imports, and structure are collected
   
3. Prompt Construction
   └─> User prompt is built with:
       - Code content
       - Dependency information
       - Explicit instructions on MDC structure
   
4. LLM Request
   └─> Messages sent to LLM:
       - System: "You are an expert code documentation specialist."
       - User: [Detailed prompt with code and instructions]
       - Response Format: MDCResponse Pydantic model
   
5. Structured Response
   └─> LLM generates JSON matching MDCResponse schema
   
6. Validation & Parsing
   └─> Response is validated against Pydantic model
   
7. MDC File Writing
   └─> MDCResponse object is written as MDC file with:
       - Frontmatter (description, globs, alwaysApply)
       - Content (markdown documentation)
```

## 6. Writing the MDC File

The `write_mdc_file()` function in `cursor_mdc_generator/code_summarization.py` converts the `MDCResponse` object to the final MDC file format:

```python
def write_mdc_file(output_path, mdc_content):
    """Write the MDC content to a file."""
    with open(output_path, "w", encoding="utf-8") as f:
        # Write the frontmatter
        f.write("---\n")
        f.write("description: {}\n".format(mdc_content.description))
        f.write("globs: {}\n".format(mdc_content.globs))
        f.write("alwaysApply: {}\n".format(str(mdc_content.always_apply).lower()))
        f.write("---\n\n")
        
        # Write the content
        f.write(mdc_content.content)
```

### Example Output:

```markdown
---
description: Utilities for LLM client interaction and response generation
globs: ["cursor_mdc_generator/llm_utils/llm_client.py"]
alwaysApply: false
---

## Overview
This file provides the core LLM client functionality for generating MDC responses...

## Key Components
- `generate_mdc_response()`: Generates structured MDC responses
- `batch_generate_mdc_responses()`: Batch processing for multiple files
...
```

## 7. Context Window Management

The system intelligently selects models based on content size:

```python
# Calculate token count
messages_tokens = len(tokenize(message_content, tokenizer))

if messages_tokens > 1000000:  # >1M tokens
    # Use chunking approach
    return await process_large_content(system_prompt, user_prompt, temperature)
elif messages_tokens > 200000:  # 200K-1M tokens
    selected_model = "gemini-2.0-flash"
elif messages_tokens > 128000:  # 128K-200K tokens
    selected_model = "claude-3-5-sonnet-latest"
else:  # <128K tokens
    selected_model = model_name  # Default: gpt-4o-mini
```

This ensures that large codebases can be processed effectively without exceeding context window limits.

## Summary

The LLM gets guidance on MDC file structure through a combination of:

1. **Pydantic Model** - Enforces the schema and provides field descriptions
2. **Explicit Prompts** - Gives detailed instructions on what to include
3. **Structured Output** - Ensures the response matches the required format
4. **System Prompt** - Sets the LLM's role and expertise

This three-layer approach ensures consistent, high-quality MDC files that provide valuable context for Cursor IDE users.
